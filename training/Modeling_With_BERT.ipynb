{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first half of this notebook trains and saves a model\n",
    "# the second half tests a saved model.\n",
    "# note: the first half saves a model with a different name then is used in the second half so the current saved model isn't overwritten.\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification, glue_convert_examples_to_features\n",
    "\n",
    "from transformers.configuration_bert import BertConfig\n",
    "\n",
    "INFILE_prefix = '../data/prelabeled/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These two optiosn can increase training time\n",
    "tf.config.optimizer.set_jit(True)\n",
    "tf.config.optimizer.set_experimental_options({'auto_mixed_precision': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Components\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "config = BertConfig(\"../models/BERT-config.json\")\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', config = config)\n",
    "\n",
    "# Data sets\n",
    "train_ds = tf.data.TFRecordDataset(INFILE_prefix + \"train47_even.tfrecord\")\n",
    "val_ds = tf.data.TFRecordDataset(INFILE_prefix + \"val47_even.tfrecord\")\n",
    "test_ds = tf.data.TFRecordDataset(INFILE_prefix + \"test47_even.tfrecord\")\n",
    "\n",
    "feat_spec = {\n",
    "    'idx' : tf.io.FixedLenFeature([], tf.int64),\n",
    "    'sentence' : tf.io.FixedLenFeature([], tf.string),\n",
    "    'label' : tf.io.FixedLenFeature([], tf.int64)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_ex(ex_proto):\n",
    "    return tf.io.parse_single_example(ex_proto, feat_spec)\n",
    "\n",
    "train_parsed = train_ds.map(parse_ex)\n",
    "val_parsed = val_ds.map(parse_ex)\n",
    "test_parsed = test_ds.map(parse_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EVAL_BATCH_SIZE = BATCH_SIZE * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/length/tweet_info.json') as json_file:\n",
    "    data_info = json.load(json_file)\n",
    "\n",
    "train_exs = data_info['train_length']\n",
    "val_exs = data_info['val_length']\n",
    "test_exs = data_info['test_length']\n",
    "\n",
    "print((train_exs, val_exs, test_exs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = glue_convert_examples_to_features(examples = train_parsed,\n",
    "                                                  tokenizer = tokenizer,\n",
    "                                                  max_length = 128,\n",
    "                                                  task = 'sst-2',\n",
    "                                                  label_list=['0','-1','1']\n",
    "                                                 )\n",
    "                                                 \n",
    "val_dataset = glue_convert_examples_to_features(examples = val_parsed,\n",
    "                                                  tokenizer = tokenizer,\n",
    "                                                  max_length = 128,\n",
    "                                                  task = 'sst-2',\n",
    "                                                  label_list=['0','-1','1']\n",
    "                                                 )    \n",
    "\n",
    "test_dataset = glue_convert_examples_to_features(examples = test_parsed,\n",
    "                                                  tokenizer = tokenizer,\n",
    "                                                  max_length = 128,\n",
    "                                                  task = 'sst-2',\n",
    "                                                  label_list=['0','-1','1']\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.shuffle(train_exs).batch(BATCH_SIZE).repeat(-1)\n",
    "val_dataset = val_dataset.shuffle(val_exs).batch(BATCH_SIZE).repeat(-1)\n",
    "test_dataset = test_dataset.batch(EVAL_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08)\n",
    "opt = tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt, 'dynamic')\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(optimizer=opt, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps = train_exs//BATCH_SIZE\n",
    "val_steps = val_exs//EVAL_BATCH_SIZE\n",
    "test_steps = test_exs//EVAL_BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset,\n",
    "                    steps_per_epoch=train_steps, \n",
    "                    epochs = 3, \n",
    "                    validation_data=val_dataset, \n",
    "                    validation_steps= val_steps, \n",
    "                    verbose = True, \n",
    "                    use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This cell saves a model with a different name than what is included to avoid over writing.\n",
    "\n",
    "# Change the path in the following cell to test out a new model\n",
    "model.save_pretrained('../models/BERT-transfer1/')\n",
    "tokenizer.save_vocabulary('../models/BERT-vocab1.dms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokenizer = BertTokenizer('../models/BERT-vocab1.dms') ### change here to try a new model\n",
    "\n",
    "new_config = BertConfig.from_json_file('../models/BERT-config0.json')\n",
    "\n",
    "new_model = TFBertForSequenceClassification.from_pretrained('../models/BERT-transfer1', config=new_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model does not need to be compiled for prediction and confusion matrix production\n",
    "\n",
    "# new_model.compile(optimizer=opt, loss=loss, metrics=[metric])\n",
    "y_preds = model.predict(test_dataset, verbose = True, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds_sm = tf.nn.softmax(y_preds)\n",
    "y_preds_argmax = tf.math.argmax(y_preds_sm, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = tf.Variable([], dtype = tf.int64)\n",
    "\n",
    "for feat, lab in test_dataset.take(-1):\n",
    "    y_true = tf.concat([y_true, lab], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pandas\n",
    "%matplotlib inline\n",
    "confusion = tf.math.confusion_matrix(y_true, y_preds_argmax).numpy()\n",
    "\n",
    "sns.heatmap(confusion, annot = True, fmt='g', cmap = plt.cm.Blues)\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# classes 0, 1, 2 refer to labels 0, -1, 1 in this model. \n",
    "# this will be changed to make more sense i nthe future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exact Training Accuracy\n",
    "m = tf.keras.metrics.Accuracy()\n",
    "m.update_state(y_true, y_preds_argmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
